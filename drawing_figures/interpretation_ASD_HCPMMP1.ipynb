{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from scipy.stats import ttest_ind, levene\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "import pandas as pd\n",
    "from statsmodels.stats.multitest import fdrcorrection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 load coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nib.load('..data/atlas/HCPMMP1_for_ABIDE.nii.gz') # original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = plotting.find_parcellation_cut_coords(labels_img=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 load interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_path = # YOUR PATH HERE (denoted as target 0)\n",
    "sub_list = list(set([i.split('_')[0] for i in os.listdir(healthy_path)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_contribution = np.zeros((len(sub_list), 180, 180))\n",
    "low_contribution = np.zeros((len(sub_list), 180, 180))\n",
    "ultralow_contribution = np.zeros((len(sub_list), 180, 180))\n",
    "\n",
    "\n",
    "for i, name in enumerate(sub_list):\n",
    "    activation_path = healthy_path+f'/{name}_att_mat_activation.json'\n",
    "    gradient_path = healthy_path+f'/{name}_att_mat_gradient.json'\n",
    "    with open(activation_path, 'r') as file:\n",
    "        activations = json.load(file)\n",
    "    with open(gradient_path, 'r') as file:\n",
    "        gradients = json.load(file)\n",
    "    \n",
    "    # high\n",
    "    high_act = activations['high_spatial_attention']\n",
    "    high_grad = gradients['high_spatial_attention']\n",
    "    high_act_mean = torch.mean(torch.tensor(high_act), dim=0)\n",
    "    high_grad_mean = torch.mean(torch.tensor(high_grad).squeeze(dim=0), dim=0)\n",
    "    contributions = torch.matmul(high_act_mean, high_grad_mean)\n",
    "    high_contribution[i, :, :] = contributions\n",
    "    \n",
    "    # low\n",
    "    low_act = activations['low_spatial_attention']\n",
    "    low_grad = gradients['low_spatial_attention']\n",
    "    low_act_mean = torch.mean(torch.tensor(low_act), dim=0)\n",
    "    low_grad_mean = torch.mean(torch.tensor(low_grad).squeeze(dim=0), dim=0)\n",
    "    contributions = torch.matmul(low_act_mean, low_grad_mean)\n",
    "    low_contribution[i, :, :] = contributions\n",
    "    \n",
    "    # ultralow\n",
    "    ultralow_act = activations['ultralow_spatial_attention']\n",
    "    ultralow_grad = gradients['ultralow_spatial_attention']\n",
    "    ultralow_act_mean = torch.mean(torch.tensor(ultralow_act), dim=0)\n",
    "    ultralow_grad_mean = torch.mean(torch.tensor(ultralow_grad).squeeze(dim=0), dim=0)\n",
    "    contributions = torch.matmul(ultralow_act_mean, ultralow_grad_mean)\n",
    "    ultralow_contribution[i, :, :] = contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASD_path = # YOUR PATH HERE (denoted as target 1)\n",
    "sub_list = list(set([i.split('_')[0] for i in os.listdir(ASD_path)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_contribution_ASD = np.zeros((len(sub_list), 180, 180))\n",
    "low_contribution_ASD = np.zeros((len(sub_list), 180, 180))\n",
    "ultralow_contribution_ASD = np.zeros((len(sub_list), 180, 180))\n",
    "\n",
    "\n",
    "for i, name in enumerate(sub_list):\n",
    "    activation_path = ASD_path+f'/{name}_att_mat_activation.json'\n",
    "    gradient_path = ASD_path+f'/{name}_att_mat_gradient.json'\n",
    "    with open(activation_path, 'r') as file:\n",
    "        activations = json.load(file)\n",
    "    with open(gradient_path, 'r') as file:\n",
    "        gradients = json.load(file)\n",
    "    \n",
    "    # high\n",
    "    high_act = activations['high_spatial_attention']\n",
    "    high_grad = gradients['high_spatial_attention']\n",
    "    high_act_mean = torch.mean(torch.tensor(high_act), dim=0)\n",
    "    high_grad_mean = torch.mean(torch.tensor(high_grad).squeeze(dim=0), dim=0)\n",
    "    contributions = torch.matmul(high_act_mean, high_grad_mean)\n",
    "    high_contribution_ASD[i, :, :] = contributions\n",
    "    \n",
    "    # low\n",
    "    low_act = activations['low_spatial_attention']\n",
    "    low_grad = gradients['low_spatial_attention']\n",
    "    low_act_mean = torch.mean(torch.tensor(low_act), dim=0)\n",
    "    low_grad_mean = torch.mean(torch.tensor(low_grad).squeeze(dim=0), dim=0)\n",
    "    contributions = torch.matmul(low_act_mean, low_grad_mean)\n",
    "    low_contribution_ASD[i, :, :] = contributions\n",
    "    \n",
    "    # ultralow\n",
    "    ultralow_act = activations['ultralow_spatial_attention']\n",
    "    ultralow_grad = gradients['ultralow_spatial_attention']\n",
    "    ultralow_act_mean = torch.mean(torch.tensor(ultralow_act), dim=0)\n",
    "    ultralow_grad_mean = torch.mean(torch.tensor(ultralow_grad).squeeze(dim=0), dim=0)\n",
    "    contributions = torch.matmul(ultralow_act_mean, ultralow_grad_mean)\n",
    "    ultralow_contribution_ASD[i, :, :] = contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 t-test between ASD and HC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1 High frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_high = np.zeros((180, 180))\n",
    "t_stats_high = np.zeros((180, 180))\n",
    "cohens_d_high = np.zeros((180, 180))\n",
    "for i in range(180):\n",
    "    for j in range(180):\n",
    "        _, p_levene = levene(high_contribution[:, i, j], high_contribution_ASD[:, i, j])\n",
    "        equal_var = True if p_levene > 0.05 else False\n",
    "        t_stat, p_value = ttest_ind(high_contribution[:, i, j], high_contribution_ASD[:, i, j], equal_var=equal_var)\n",
    "        cohens_d = (np.mean(high_contribution[:, i, j]) - np.mean(high_contribution_ASD[:, i, j])) / np.sqrt(((len(high_contribution[:, i, j]) - 1) * np.var(high_contribution[:, i, j]) + (len(high_contribution_ASD[:, i, j]) - 1) * np.var(high_contribution_ASD[:, i, j])) / (len(high_contribution[:, i, j]) + len(high_contribution_ASD[:, i, j]) - 2))\n",
    "        p_values_high[i, j] = p_value\n",
    "        t_stats_high[i, j] = t_stat\n",
    "        cohens_d_high[i, j] = np.abs(cohens_d)\n",
    "        \n",
    "# FDR correction\n",
    "p_values_flat = p_values_high.flatten()\n",
    "_, p_values_corrected_flat = fdrcorrection(p_values_flat, alpha=0.05)\n",
    "p_values_high_corrected = p_values_corrected_flat.reshape(180, 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistically significant connectivity which are responsible for distinguishing two groups\n",
    "filtered_matrix_high = np.where(p_values_high_corrected <= 0.05, 1, 0)\n",
    "sns.heatmap(filtered_matrix_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# effect size\n",
    "sns.heatmap(cohens_d_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(cohens_d_high*filtered_matrix_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sign_high = np.where(t_stats_high < 0, -1, 1)\n",
    "sns.heatmap(cohens_d_high*filtered_matrix_high*t_sign_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2 low frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_low = np.zeros((180, 180))\n",
    "t_stats_low = np.zeros((180, 180))\n",
    "cohens_d_low = np.zeros((180, 180))\n",
    "for i in range(180):\n",
    "    for j in range(180):\n",
    "        _, p_levene = levene(low_contribution[:, i, j], low_contribution_ASD[:, i, j])\n",
    "        equal_var = True if p_levene > 0.05 else False\n",
    "        t_stat, p_value = ttest_ind(low_contribution[:, i, j], low_contribution_ASD[:, i, j], equal_var=equal_var)\n",
    "        cohens_d = (np.mean(low_contribution[:, i, j]) - np.mean(low_contribution_ASD[:, i, j])) / np.sqrt(((len(low_contribution[:, i, j]) - 1) * np.var(low_contribution[:, i, j]) + (len(low_contribution_ASD[:, i, j]) - 1) * np.var(low_contribution_ASD[:, i, j])) / (len(low_contribution[:, i, j]) + len(low_contribution_ASD[:, i, j]) - 2))\n",
    "        p_values_low[i, j] = p_value\n",
    "        t_stats_low[i, j] = t_stat\n",
    "        cohens_d_low[i, j] = np.abs(cohens_d)\n",
    "        \n",
    "# FDR correction\n",
    "p_values_flat = p_values_low.flatten()\n",
    "_, p_values_corrected_flat = fdrcorrection(p_values_flat, alpha=0.05)\n",
    "p_values_low_corrected = p_values_corrected_flat.reshape(180, 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistically significant connectivity which are responsible for distinguishing two groups\n",
    "filtered_matrix_low = np.where(p_values_low_corrected <= 0.05, 1, 0)\n",
    "sns.heatmap(filtered_matrix_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effect size\n",
    "sns.heatmap(cohens_d_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(cohens_d_low*filtered_matrix_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sign_low = np.where(t_stats_low < 0, -1, 1) # reversed\n",
    "sns.heatmap(cohens_d_low*filtered_matrix_low*t_sign_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3 ultralow frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_ultralow = np.zeros((180, 180))\n",
    "t_stats_ultralow = np.zeros((180, 180))\n",
    "cohens_d_ultralow = np.zeros((180, 180))\n",
    "for i in range(180):\n",
    "    for j in range(180):\n",
    "        _, p_levene = levene(ultralow_contribution[:, i, j], ultralow_contribution_ASD[:, i, j])\n",
    "        equal_var = True if p_levene > 0.05 else False\n",
    "        t_stat, p_value = ttest_ind(ultralow_contribution[:, i, j], ultralow_contribution_ASD[:, i, j], equal_var=equal_var)\n",
    "        cohens_d = (np.mean(ultralow_contribution[:, i, j]) - np.mean(ultralow_contribution_ASD[:, i, j])) / np.sqrt(((len(ultralow_contribution[:, i, j]) - 1) * np.var(ultralow_contribution[:, i, j]) + (len(ultralow_contribution_ASD[:, i, j]) - 1) * np.var(ultralow_contribution_ASD[:, i, j])) / (len(ultralow_contribution[:, i, j]) + len(ultralow_contribution_ASD[:, i, j]) - 2))\n",
    "        p_values_ultralow[i, j] = p_value\n",
    "        t_stats_ultralow[i, j] = t_stat\n",
    "        cohens_d_ultralow[i, j] = np.abs(cohens_d)\n",
    "        \n",
    "# FDR correction\n",
    "p_values_flat = p_values_ultralow.flatten()\n",
    "_, p_values_corrected_flat = fdrcorrection(p_values_flat, alpha=0.05)\n",
    "p_values_ultralow_corrected = p_values_corrected_flat.reshape(180, 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistically significant connectivity which are responsible for distinguishing two groups\n",
    "filtered_matrix_ultralow = np.where(p_values_ultralow_corrected <= 0.05, 1, 0)\n",
    "sns.heatmap(filtered_matrix_ultralow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effect size\n",
    "sns.heatmap(cohens_d_ultralow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(cohens_d_ultralow*filtered_matrix_ultralow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sign_ultralow = np.where(t_stats_ultralow < 0, -1, 1)\n",
    "sns.heatmap(cohens_d_ultralow*filtered_matrix_ultralow*t_sign_ultralow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Load atlas meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all hemispheres\n",
    "atlas_info = pd.read_csv('../data/coordinates/HCP-MMP1_UniqueRegionList.csv', encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_info = atlas_info.replace(r'\\n', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 Visualization on a glass brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-1 High frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_elements = cohens_d_high*filtered_matrix_high\n",
    "flattened_indices = np.argsort(-significant_elements, axis=None)[:100]\n",
    "filtered_matrix_high_mask = np.zeros_like(filtered_matrix_high)\n",
    "rows, cols = np.unravel_index(flattened_indices, filtered_matrix_high.shape)\n",
    "\n",
    "for row, col in zip(rows, cols):\n",
    "    if t_sign_high[row, col]==abs(t_sign_high[row, col]):\n",
    "        filtered_matrix_high_mask[row, col] = 1  \n",
    "    else:\n",
    "        filtered_matrix_high_mask[row, col] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(filtered_matrix_high_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original -> symmetric\n",
    "\n",
    "for i, j in zip(rows, cols):\n",
    "    p_value = p_values_high[i, j]\n",
    "    cohen = cohens_d_high[i, j]\n",
    "    row = atlas_info[atlas_info['regionID'] == i+1][['regionLongName']].values[0][0]\n",
    "    col = atlas_info[atlas_info['regionID'] == j+1][['regionLongName']].values[0][0]\n",
    "    if filtered_matrix_high_mask[i, j] > 0:\n",
    "        description = 'HC'\n",
    "    elif filtered_matrix_high_mask[i, j] < 0:\n",
    "        description = 'ASD'\n",
    "    else:\n",
    "        description = 'None'\n",
    "    print(row.replace('_', ' ')[:-2]+','+col.replace('_', ' ')[:-2]+','+description+','+str(round(p_value, 3))+','+str(round(cohen, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = np.zeros((360, 360))\n",
    "answer[:180, :180] = filtered_matrix_high_mask\n",
    "answer[180:, 180:] = filtered_matrix_high_mask\n",
    "sns.heatmap(answer)\n",
    "\n",
    "view = plotting.view_connectome(answer *(-1),coordinates, node_size=3.0)\n",
    "view.save_as_html('reversed_color_symmetric_ASD_ROI_180_high_freq_sign.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_elements = cohens_d_low*filtered_matrix_low\n",
    "flattened_indices = np.argsort(-significant_elements, axis=None)[:100]\n",
    "\n",
    "filtered_matrix_low_mask = np.zeros_like(filtered_matrix_low)\n",
    "\n",
    "rows, cols = np.unravel_index(flattened_indices, filtered_matrix_low.shape)\n",
    "\n",
    "for row, col in zip(rows, cols):\n",
    "    if t_stats_low[row, col]==abs(t_stats_low[row, col]):\n",
    "        filtered_matrix_low_mask[row, col] = 1\n",
    "    else:\n",
    "        filtered_matrix_low_mask[row, col] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(filtered_matrix_low_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original -> symmetric\n",
    "\n",
    "for i, j in zip(rows, cols):\n",
    "    p_value = p_values_low[i, j]\n",
    "    cohen = cohens_d_low[i, j]\n",
    "    row = atlas_info[atlas_info['regionID'] == i+1][['regionLongName']].values[0][0]\n",
    "    col = atlas_info[atlas_info['regionID'] == j+1][['regionLongName']].values[0][0]\n",
    "    if filtered_matrix_low_mask[i, j] > 0:\n",
    "        description = 'HC'\n",
    "    elif filtered_matrix_low_mask[i, j] < 0:\n",
    "        description = 'ASD'\n",
    "    else:\n",
    "        description = 'None'\n",
    "    if p_value <= 0.05:\n",
    "        print(row.replace('_', ' ')[:-2]+','+col.replace('_', ' ')[:-2]+','+description+','+str(round(p_value, 3))+','+str(round(cohen, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = np.zeros((360, 360))\n",
    "answer[:180, :180] = filtered_matrix_low_mask\n",
    "answer[180:, 180:] = filtered_matrix_low_mask\n",
    "sns.heatmap(answer)\n",
    "\n",
    "view = plotting.view_connectome(answer*(-1),coordinates, node_size=3.0)\n",
    "view.save_as_html('reversed_color_symmetric_ASD_ROI_180_low_freq_sign.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-3 ultralow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_elements = cohens_d_ultralow*filtered_matrix_ultralow\n",
    "flattened_indices = np.argsort(-significant_elements, axis=None)[:100]\n",
    "\n",
    "filtered_matrix_ultralow_mask = np.zeros_like(filtered_matrix_ultralow)\n",
    "\n",
    "rows, cols = np.unravel_index(flattened_indices, filtered_matrix_ultralow.shape)\n",
    "\n",
    "for row, col in zip(rows, cols):\n",
    "    print(significant_elements[row, col], t_sign_ultralow[row, col])\n",
    "    if t_sign_ultralow[row, col]==abs(t_sign_ultralow[row, col]):\n",
    "        filtered_matrix_ultralow_mask[row, col] = 1\n",
    "    else:\n",
    "        filtered_matrix_ultralow_mask[row, col] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(filtered_matrix_ultralow_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original -> symmetric\n",
    "\n",
    "for i, j in zip(rows, cols):\n",
    "    p_value = p_values_ultralow[i, j]\n",
    "    cohen = cohens_d_ultralow[i, j]\n",
    "    row = atlas_info[atlas_info['regionID'] == i+1][['regionLongName']].values[0][0]\n",
    "    col = atlas_info[atlas_info['regionID'] == j+1][['regionLongName']].values[0][0]\n",
    "    if filtered_matrix_ultralow_mask[i, j] > 0:\n",
    "        description = 'HC'\n",
    "    elif filtered_matrix_ultralow_mask[i, j] < 0:\n",
    "        description = 'ASD'\n",
    "    else:\n",
    "        description = 'None'\n",
    "    print(row.replace('_', ' ')[:-2]+','+col.replace('_', ' ')[:-2]+','+description+','+str(round(p_value, 3))+','+str(round(cohen, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = np.zeros((360, 360))\n",
    "answer[:180, :180] = filtered_matrix_ultralow_mask\n",
    "answer[180:, 180:] = filtered_matrix_ultralow_mask\n",
    "sns.heatmap(answer)\n",
    "\n",
    "view = plotting.view_connectome(answer*(-1),coordinates, node_size=3.0)\n",
    "view.save_as_html('reversed_color_symmetric_ASD_ROI_180_ultralow_freq_sign.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "divfreq",
   "language": "python",
   "name": "divfreq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
